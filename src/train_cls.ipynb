{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exotic-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rs = []\n",
    "sub_dir = os.listdir('../data/pill_recog')\n",
    "\n",
    "for classname in sub_dir:\n",
    "    if classname == '107':\n",
    "        continue\n",
    "    for filename in os.listdir('../data/pill_recog/' + classname):\n",
    "        filepath = os.path.join(classname, filename)\n",
    "        list_rs.append([filepath, classname])\n",
    "df = pd.DataFrame(list_rs, columns=['filepath', 'label'])\n",
    "df.to_csv('../data/pill_recog.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affiliated-adapter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            filepath  label\n",
      "0  ../data/crop/train_crop/74/VAIPE_P_1045_1_pill...     74\n",
      "1  ../data/crop/train_crop/74/VAIPE_P_63_18_pill1...     74\n",
      "2  ../data/crop/train_crop/74/VAIPE_P_304_21_pill...     74\n",
      "3  ../data/crop/train_crop/74/VAIPE_P_768_3_pill7...     74\n",
      "4  ../data/crop/train_crop/74/VAIPE_P_889_1_pill6...     74\n",
      "                                          filepath  label\n",
      "0  ../data/crop/val_crop/74/VAIPE_P_105_1_2469.jpg     74\n",
      "1  ../data/crop/val_crop/74/VAIPE_P_146_0_3030.jpg     74\n",
      "2  ../data/crop/val_crop/74/VAIPE_P_134_1_2906.jpg     74\n",
      "3  ../data/crop/val_crop/74/VAIPE_P_134_0_2905.jpg     74\n",
      "4  ../data/crop/val_crop/74/VAIPE_P_105_1_2470.jpg     74\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('../data/crop/train_crop.csv')\n",
    "print(df1.head())\n",
    "df2 = pd.read_csv('../data/crop/val_crop.csv')\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "piano-works",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/crop/train_crop/74/VAIPE_P_1045_1_pill...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/crop/train_crop/74/VAIPE_P_63_18_pill1...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/crop/train_crop/74/VAIPE_P_304_21_pill...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/crop/train_crop/74/VAIPE_P_768_3_pill7...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/crop/train_crop/74/VAIPE_P_889_1_pill6...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath  label\n",
       "0  ../data/crop/train_crop/74/VAIPE_P_1045_1_pill...     74\n",
       "1  ../data/crop/train_crop/74/VAIPE_P_63_18_pill1...     74\n",
       "2  ../data/crop/train_crop/74/VAIPE_P_304_21_pill...     74\n",
       "3  ../data/crop/train_crop/74/VAIPE_P_768_3_pill7...     74\n",
       "4  ../data/crop/train_crop/74/VAIPE_P_889_1_pill6...     74"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [df1, df2]\n",
    "df = pd.concat(frames).reset_index(drop=True)\n",
    "len(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "precious-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def get_img(path):\n",
    "    im_bgr = cv2.imread(path)\n",
    "    im_rgb = im_bgr[:, :, ::-1]\n",
    "    return im_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "affiliated-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class PillDataset(Dataset):\n",
    "    def __init__(self, df, data_root, \n",
    "                 transforms=None, \n",
    "                 output_label=True, \n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.transforms = transforms\n",
    "        self.data_root = data_root\n",
    "        \n",
    "        self.output_label = output_label\n",
    "        if output_label:\n",
    "            self.labels = self.df['label'].values\n",
    "            \n",
    "        \"\"\"\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        for filename in os.listdir(os.path.join(data_root, sub_dir)):\n",
    "            filepath = os.path.join(data_root, sub_dir, filename)\n",
    "            images.append(filepath)\n",
    "            labels.append(sub_dir)\n",
    "            \n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        \"\"\" \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        if self.output_label:\n",
    "            target = self.labels[index]\n",
    "            # target = dict_label[target]\n",
    "        imgname = self.df.loc[index]['filepath']\n",
    "        img  = get_img(f\"{self.data_root}/{self.df.loc[index]['filepath']}\")\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "                            \n",
    "        # do label smoothing\n",
    "        if self.output_label:\n",
    "            return img, target\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "homeless-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "velvet-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return Compose([\n",
    "            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
    "            Transpose(p=0.5),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            # VerticalFlip(p=0.5),\n",
    "            # RandomRotate90(p=0.5),\n",
    "            # ShiftScaleRotate(shift_limit=0.0, scale_limit=0.3, rotate_limit=10, border_mode=0, p=0.7),\n",
    "            # HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            # RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            CoarseDropout(p=0.5),\n",
    "            Cutout(p=0.1),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "  \n",
    "        \n",
    "def get_valid_transforms():\n",
    "    return Compose([\n",
    "            Resize(CFG['img_size'], CFG['img_size']),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "related-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: vit_large_patch16_384\n",
    "CFG = {\n",
    "    'fold_num': 10,\n",
    "    'seed': 719,\n",
    "    'model_arch': 'tf_efficientnet_b7_ns',\n",
    "    'img_size': 224,\n",
    "    'epochs': 10,\n",
    "    'train_bs': 8,\n",
    "    'valid_bs': 16,\n",
    "    'T_0': 10,\n",
    "    'lr': 1e-4,\n",
    "    'min_lr': 1e-6,\n",
    "    'weight_decay':1e-6,\n",
    "    'num_workers': 4,\n",
    "    'accum_iter': 2,\n",
    "    'verbose_step': 1,\n",
    "    'device': 'cuda:0'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hollow-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "national-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PillClassifier(nn.Module):\n",
    "    def __init__(self, model_arch, n_class, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        # n_features = self.model.head.out_features\n",
    "        # n_features = self.model.head.fc.out_features\n",
    "        # self.out_layer = nn.Linear(n_features, n_class)\n",
    "        n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(n_features, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        # x = self.out_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adult-survivor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(df, trn_idx, val_idx, data_root='../data/'):\n",
    "    \n",
    "    # from catalyst.data.sampler import BalanceClassSampler\n",
    "    \n",
    "    train_ = df.loc[trn_idx,:].reset_index(drop=True)\n",
    "    valid_ = df.loc[val_idx,:].reset_index(drop=True)\n",
    "        \n",
    "    train_ds = PillDataset(train_, data_root, transforms=get_train_transforms(),\n",
    "                              output_label=True)\n",
    "    valid_ds = PillDataset(valid_, data_root, transforms=get_valid_transforms(),output_label=True)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=CFG['train_bs'],\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        shuffle=True, \n",
    "        num_workers=CFG['num_workers'],\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        valid_ds,\n",
    "        batch_size=CFG['valid_bs'],\n",
    "        num_workers=CFG['num_workers'],\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train_one_epoch(epoch, model, loss_fn, optimizer, train_loader, device, logger, scheduler=None, schd_batch_update=False):\n",
    "    model.train()\n",
    "\n",
    "    t = time.time()\n",
    "    running_loss = None\n",
    "    \n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for step, (imgs, image_labels) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        image_labels = image_labels.to(device).long()\n",
    "\n",
    "        with autocast():\n",
    "            image_preds = model(imgs)\n",
    "\n",
    "            loss = loss_fn(image_preds, image_labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if running_loss is None:\n",
    "                running_loss = loss.item()\n",
    "            else:\n",
    "                running_loss = running_loss * .99 + loss.item() * .01\n",
    "\n",
    "            if ((step + 1) %  CFG['accum_iter'] == 0) or ((step + 1) == len(train_loader)):\n",
    "                # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n",
    "\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad() \n",
    "                \n",
    "                if scheduler is not None and schd_batch_update:\n",
    "                    scheduler.step()\n",
    "\n",
    "            if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(train_loader)):\n",
    "                description = f'epoch {epoch} loss: {running_loss:.4f}'\n",
    "                # logger.info('Epoch {} loss: {}'.format(epoch, running_loss))\n",
    "                pbar.set_description(description)\n",
    "                \n",
    "    if scheduler is not None and not schd_batch_update:\n",
    "        scheduler.step()\n",
    "        \n",
    "def valid_one_epoch(epoch, model, loss_fn, val_loader, device, logger, scheduler=None, schd_loss_update=False):\n",
    "    model.eval()\n",
    "\n",
    "    t = time.time()\n",
    "    loss_sum = 0\n",
    "    sample_num = 0\n",
    "    image_preds_all = []\n",
    "    image_targets_all = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n",
    "    for step, (imgs, image_labels) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        image_labels = image_labels.to(device).long()\n",
    "        \n",
    "        image_preds = model(imgs)\n",
    "        image_preds_all += [torch.argmax(image_preds, 1).detach().cpu().numpy()]\n",
    "        image_targets_all += [image_labels.detach().cpu().numpy()]\n",
    "        \n",
    "        loss = loss_fn(image_preds, image_labels)\n",
    "        \n",
    "        loss_sum += loss.item()*image_labels.shape[0]\n",
    "        sample_num += image_labels.shape[0]  \n",
    "\n",
    "        if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(val_loader)):\n",
    "            description = f'epoch {epoch} loss: {loss_sum/sample_num:.4f}'\n",
    "            # logger.info('Epoch {} loss: {}'.format(epoch, loss_sum/sample_num))\n",
    "            pbar.set_description(description)\n",
    "    \n",
    "    image_preds_all = np.concatenate(image_preds_all)\n",
    "    image_targets_all = np.concatenate(image_targets_all)\n",
    "    print(f'validation multi-class accuracy = {(image_preds_all==image_targets_all).mean():.4f}')\n",
    "    logger.info('Validation multi-class accuracy: {}'.format((image_preds_all == image_targets_all).mean()))\n",
    "    if scheduler is not None:\n",
    "        if schd_loss_update:\n",
    "            scheduler.step(loss_sum/sample_num)\n",
    "        else:\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aggregate-candle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/crop/train_crop/74/VAIPE_P_1045_1_pill...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/crop/train_crop/74/VAIPE_P_63_18_pill1...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/crop/train_crop/74/VAIPE_P_304_21_pill...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/crop/train_crop/74/VAIPE_P_768_3_pill7...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/crop/train_crop/74/VAIPE_P_889_1_pill6...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath  label\n",
       "0  ../data/crop/train_crop/74/VAIPE_P_1045_1_pill...     74\n",
       "1  ../data/crop/train_crop/74/VAIPE_P_63_18_pill1...     74\n",
       "2  ../data/crop/train_crop/74/VAIPE_P_304_21_pill...     74\n",
       "3  ../data/crop/train_crop/74/VAIPE_P_768_3_pill7...     74\n",
       "4  ../data/crop/train_crop/74/VAIPE_P_889_1_pill6...     74"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = df\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "persistent-conference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-08-31 10:09:20,093 - urllib3.connectionpool - DEBUG] - Starting new HTTPS connection (1): raw.githubusercontent.com:443\n",
      "[2022-08-31 10:09:20,252 - urllib3.connectionpool - DEBUG] - https://raw.githubusercontent.com:443 \"GET /cuongngm/logult/why/config/logger_config.json HTTP/1.1\" 200 372\n",
      "[2022-08-31 10:09:20,270 - root - INFO] - Training with 0 started\n",
      "[2022-08-31 10:09:20,271 - root - INFO] - Found dataset with 25056 train sample, 2785 val sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/cuongnm1/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/data/cuongnm1/anaconda3/envs/pytorch/lib/python3.6/site-packages/albumentations/augmentations/transforms.py:691: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-08-31 10:09:21,457 - timm.models.helpers - INFO] - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b7_ns-1dbc32de.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.8422: 100%|██████████| 3132/3132 [11:57<00:00,  4.37it/s]\n",
      "epoch 0 loss: 0.4714: 100%|██████████| 175/175 [00:21<00:00,  8.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8740\n",
      "[2022-08-31 10:21:42,624 - root - INFO] - Validation multi-class accuracy: 0.873967684021544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 1 loss: 0.5770: 100%|██████████| 3132/3132 [11:40<00:00,  4.47it/s]\n",
      "epoch 1 loss: 0.3379: 100%|██████████| 175/175 [00:19<00:00,  8.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9063\n",
      "[2022-08-31 10:33:43,844 - root - INFO] - Validation multi-class accuracy: 0.9062836624775583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 2 loss: 0.4869: 100%|██████████| 3132/3132 [12:07<00:00,  4.30it/s]\n",
      "epoch 2 loss: 0.2668: 100%|██████████| 175/175 [00:19<00:00,  9.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9189\n",
      "[2022-08-31 10:46:12,052 - root - INFO] - Validation multi-class accuracy: 0.918850987432675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 3 loss: 0.4544: 100%|██████████| 3132/3132 [11:43<00:00,  4.45it/s]\n",
      "epoch 3 loss: 0.2257: 100%|██████████| 175/175 [00:19<00:00,  9.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9300\n",
      "[2022-08-31 10:58:15,804 - root - INFO] - Validation multi-class accuracy: 0.9299820466786356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 4 loss: 0.4121: 100%|██████████| 3132/3132 [11:39<00:00,  4.48it/s]\n",
      "epoch 4 loss: 0.2348: 100%|██████████| 175/175 [00:19<00:00,  8.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9246\n",
      "[2022-08-31 11:10:16,175 - root - INFO] - Validation multi-class accuracy: 0.9245960502692998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 5 loss: 0.2816: 100%|██████████| 3132/3132 [12:03<00:00,  4.33it/s]\n",
      "epoch 5 loss: 0.2026: 100%|██████████| 175/175 [00:19<00:00,  9.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9364\n",
      "[2022-08-31 11:22:40,172 - root - INFO] - Validation multi-class accuracy: 0.9364452423698384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 6 loss: 0.2926: 100%|██████████| 3132/3132 [11:39<00:00,  4.48it/s]\n",
      "epoch 6 loss: 0.1886: 100%|██████████| 175/175 [00:19<00:00,  9.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9422\n",
      "[2022-08-31 11:34:40,374 - root - INFO] - Validation multi-class accuracy: 0.9421903052064632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 7 loss: 0.2222: 100%|██████████| 3132/3132 [11:57<00:00,  4.37it/s]\n",
      "epoch 7 loss: 0.1809: 100%|██████████| 175/175 [00:19<00:00,  8.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9454\n",
      "[2022-08-31 11:46:58,312 - root - INFO] - Validation multi-class accuracy: 0.9454219030520646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 8 loss: 0.2375: 100%|██████████| 3132/3132 [12:00<00:00,  4.35it/s]\n",
      "epoch 8 loss: 0.1658: 100%|██████████| 175/175 [00:19<00:00,  9.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9494\n",
      "[2022-08-31 11:59:19,162 - root - INFO] - Validation multi-class accuracy: 0.9493716337522442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 9 loss: 0.2573: 100%|██████████| 3132/3132 [11:45<00:00,  4.44it/s]\n",
      "epoch 9 loss: 0.1535: 100%|██████████| 175/175 [00:19<00:00,  8.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9508\n",
      "[2022-08-31 12:11:25,467 - root - INFO] - Validation multi-class accuracy: 0.9508078994614003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-08-31 12:11:26,123 - root - INFO] - Training with 1 started\n",
      "[2022-08-31 12:11:26,124 - root - INFO] - Found dataset with 25057 train sample, 2784 val sample\n",
      "[2022-08-31 12:11:27,373 - timm.models.helpers - INFO] - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b7_ns-1dbc32de.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.8877: 100%|██████████| 3133/3133 [11:57<00:00,  4.36it/s]\n",
      "epoch 0 loss: 0.4821: 100%|██████████| 174/174 [00:19<00:00,  9.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.8635\n",
      "[2022-08-31 12:23:47,360 - root - INFO] - Validation multi-class accuracy: 0.8635057471264368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 1 loss: 0.6066: 100%|██████████| 3133/3133 [11:43<00:00,  4.45it/s]\n",
      "epoch 1 loss: 0.3555: 100%|██████████| 174/174 [00:19<00:00,  9.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9030\n",
      "[2022-08-31 12:35:51,743 - root - INFO] - Validation multi-class accuracy: 0.9030172413793104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 2 loss: 0.5284: 100%|██████████| 3133/3133 [11:48<00:00,  4.42it/s]\n",
      "epoch 2 loss: 0.2944: 100%|██████████| 174/174 [00:19<00:00,  9.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9159\n",
      "[2022-08-31 12:48:00,915 - root - INFO] - Validation multi-class accuracy: 0.915948275862069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 3 loss: 0.5080: 100%|██████████| 3133/3133 [11:45<00:00,  4.44it/s]\n",
      "epoch 3 loss: 0.2744: 100%|██████████| 174/174 [00:19<00:00,  9.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9221\n",
      "[2022-08-31 13:00:07,286 - root - INFO] - Validation multi-class accuracy: 0.9220545977011494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 4 loss: 0.4009: 100%|██████████| 3133/3133 [11:58<00:00,  4.36it/s]\n",
      "epoch 4 loss: 0.2595: 100%|██████████| 174/174 [00:19<00:00,  8.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9253\n",
      "[2022-08-31 13:12:26,894 - root - INFO] - Validation multi-class accuracy: 0.9252873563218391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 5 loss: 0.3492: 100%|██████████| 3133/3133 [11:47<00:00,  4.43it/s]\n",
      "epoch 5 loss: 0.2084: 100%|██████████| 174/174 [00:19<00:00,  9.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9418\n",
      "[2022-08-31 13:24:35,294 - root - INFO] - Validation multi-class accuracy: 0.9418103448275862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 6 loss: 0.2582: 100%|██████████| 3133/3133 [11:50<00:00,  4.41it/s]\n",
      "epoch 6 loss: 0.2174: 100%|██████████| 174/174 [00:20<00:00,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9411\n",
      "[2022-08-31 13:36:47,397 - root - INFO] - Validation multi-class accuracy: 0.9410919540229885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 7 loss: 0.2569: 100%|██████████| 3133/3133 [12:01<00:00,  4.34it/s]\n",
      "epoch 7 loss: 0.1987: 100%|██████████| 174/174 [00:20<00:00,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9418\n",
      "[2022-08-31 13:49:10,372 - root - INFO] - Validation multi-class accuracy: 0.9418103448275862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 8 loss: 0.2871: 100%|██████████| 3133/3133 [11:35<00:00,  4.50it/s]\n",
      "epoch 8 loss: 0.2105: 100%|██████████| 174/174 [00:19<00:00,  8.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9418\n",
      "[2022-08-31 14:01:06,797 - root - INFO] - Validation multi-class accuracy: 0.9418103448275862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 9 loss: 0.2606: 100%|██████████| 3133/3133 [11:49<00:00,  4.41it/s]\n",
      "epoch 9 loss: 0.1972: 100%|██████████| 174/174 [00:19<00:00,  8.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation multi-class accuracy = 0.9440\n",
      "[2022-08-31 14:13:17,521 - root - INFO] - Validation multi-class accuracy: 0.9439655172413793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-08-31 14:13:18,335 - root - INFO] - Training with 2 started\n",
      "[2022-08-31 14:13:18,336 - root - INFO] - Found dataset with 25057 train sample, 2784 val sample\n",
      "[2022-08-31 14:13:19,682 - timm.models.helpers - INFO] - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b7_ns-1dbc32de.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 4.6852:   1%|          | 27/3133 [00:06<12:22,  4.19it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-0688813f9a99>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch, model, loss_fn, optimizer, train_loader, device, logger, scheduler, schd_batch_update)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m.99\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m  \u001b[0mCFG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accum_iter'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from pathlib import Path\n",
    "from logult import setup_log\n",
    "logger = setup_log(save_dir='saved/final')\n",
    "save_path = 'weights/' + CFG['model_arch'] + '_final' \n",
    "Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "     # for training only, need nightly build pytorch\n",
    "    seed_everything(CFG['seed'])\n",
    "    \n",
    "    stratifiedKFold = StratifiedKFold(n_splits=CFG['fold_num'], shuffle=True, random_state=CFG['seed'])\n",
    "    folds = stratifiedKFold.split(np.arange(train.shape[0]), train.label.values)\n",
    "    \n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds):\n",
    "        # we'll train fold 0 first\n",
    "        # if fold > 0:\n",
    "        #     break\n",
    "\n",
    "        logger.info(f'Training with {fold} started')\n",
    "        logger.info('Found dataset with {} train sample, {} val sample'.format(len(trn_idx), len(val_idx)))\n",
    "        # print(len(trn_idx), len(val_idx))\n",
    "\n",
    "        train_loader, val_loader = prepare_dataloader(train, trn_idx, val_idx, \n",
    "                                                      data_root='./')\n",
    "        \n",
    "        device = torch.device(CFG['device'])\n",
    "        \n",
    "        model = PillClassifier(CFG['model_arch'], train.label.nunique(), pretrained=True).to(device)\n",
    "        scaler = GradScaler()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'])\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=CFG['T_0'], T_mult=1, eta_min=CFG['min_lr'], last_epoch=-1)\n",
    "\n",
    "        loss_tr = nn.CrossEntropyLoss().to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "        for epoch in range(CFG['epochs']):\n",
    "            train_one_epoch(epoch, model, loss_tr, optimizer, train_loader, device, logger,\n",
    "                            scheduler=scheduler, schd_batch_update=False)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                valid_one_epoch(epoch, model, loss_fn, val_loader, device, logger,\n",
    "                                scheduler=None, schd_loss_update=False)\n",
    "        \n",
    "            torch.save(model.state_dict(), f\"weights/{CFG['model_arch']}_final/fold_{fold}_{epoch}.pth\")\n",
    "\n",
    "        del model, optimizer, train_loader, val_loader, scheduler, scaler\n",
    "        torch.cuda.empty_cache()\n",
    "        # if fold == 2:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-current",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
